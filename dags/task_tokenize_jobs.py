import asyncio
import json
import re
import aiohttp
import certifi
import ssl
from airflow.hooks.base import BaseHook
from utils import save_json_data, generate_timestamped_filename, load_json_data
from tqdm.asyncio import tqdm

# --- ì„¤ì • ---
MODEL_NAME = "deepseek/deepseek-r1-0528-qwen3-8b"
MAX_CONCURRENT_REQUESTS = 10  # ë™ì‹œ ìš”ì²­ ìˆ˜ ì œì–´
MAX_RETRIES = 3
RETRY_DELAY = 2

# --- 2ì°¨ ì „ì²˜ë¦¬ í•¨ìˆ˜ ---
def clean_and_standardize_keyword(keyword):
    """ê°œë³„ í‚¤ì›Œë“œë¥¼ í‘œì¤€í™”í•˜ê³  ì •ë¦¬í•©ë‹ˆë‹¤."""
    if not isinstance(keyword, str):
        return ""
    cleaned = keyword.lower().strip().replace(' ', '-')
    cleaned = re.sub(r'[^a-z0-9ê°€-í£-]', '', cleaned)
    return cleaned

# --- LLM í˜¸ì¶œ í•¨ìˆ˜ ---
async def tokenize_with_llm(session, semaphore, text, section_name, job_id):
    """ë¹„ë™ê¸°ì ìœ¼ë¡œ LLMì„ í˜¸ì¶œí•˜ì—¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    async with semaphore:
        if not isinstance(text, str) or not text.strip():
            return []

        prompt = f"""
            # ì§€ì‹œì‚¬í•­
            ë‹¹ì‹ ì€ ì±„ìš© ê³µê³ ë¥¼ ë¶„ì„í•˜ì—¬ í•µì‹¬ í‚¤ì›Œë“œë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
            1.  í•µì‹¬ í‚¤ì›Œë“œë¥¼ 'ëª…ì‚¬' ìœ„ì£¼ë¡œ ê°€ì¥ ì§§ê²Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
            2.  '~ê²½í—˜', '~ìš°ëŒ€' ë“±ì˜ ë¶ˆí•„ìš”í•œ ë‹¨ì–´ëŠ” ì œê±°í•©ë‹ˆë‹¤.
            3.  ê²°ê³¼ëŠ” "keywords" ë¼ëŠ” í‚¤ë¥¼ ê°€ì§„ JSON ê°ì²´ ì•ˆì— ë°°ì—´(array)ë¡œ ë‹´ì•„ì£¼ì„¸ìš”.
            
            # ì˜ˆì‹œ
            - ì…ë ¥ í…ìŠ¤íŠ¸: "â€¢ C/C++ì„ í™œìš©í•œ í”„ë¡œì íŠ¸ ê°œë°œ ê²½í—˜ì´ ìˆìœ¼ì‹  ë¶„"
            - ì¶œë ¥ ê²°ê³¼: {{"keywords": ["C/C++", "í”„ë¡œì íŠ¸"]}}

            # ë¶„ì„í•  í…ìŠ¤íŠ¸ ({section_name})
            ---
            {text}
            ---
        """
        
        connection = BaseHook.get_connection('openrouter_default')
        api_key = connection.password
        base_url = connection.host
        
        headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
        payload = {
            "model": MODEL_NAME,
            "messages": [{"role": "user", "content": prompt}],
            "response_format": {"type": "json_object"}
        }
        
        for attempt in range(MAX_RETRIES):
            try:
                async with session.post(f"{base_url}/chat/completions", headers=headers, json=payload, timeout=aiohttp.ClientTimeout(total=180)) as response:
                    response.raise_for_status()
                    result = await response.json()
                    json_content = json.loads(result['choices'][0]['message']['content'])
                    return json_content.get("keywords", [])
            except Exception as e:
                if attempt + 1 == MAX_RETRIES:
                    print(f"\nğŸš¨ [ID:{job_id}] ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨. ìµœì¢… ì˜¤ë¥˜: {e}")
                    return ["ìµœì¢… API ì˜¤ë¥˜ ë°œìƒ"]
                await asyncio.sleep(RETRY_DELAY)

# --- ë©”ì¸ ë¹„ë™ê¸° ì²˜ë¦¬ í•¨ìˆ˜ ---
async def process_jobs_main(input_path):
    try:
        data = load_json_data(input_path)
        if not data:
            print("ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
    except Exception as e:
        print(f"ğŸš¨ ì˜¤ë¥˜: '{input_path}' íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨. ì˜¤ë¥˜: {e}")
        return None

    print(f"âœ… ì´ {len(data)}ê°œì˜ ê³µê³ ì— ëŒ€í•œ í† í°í™” ë° í›„ì²˜ë¦¬ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)
    ssl_context = ssl.create_default_context(cafile=certifi.where())
    
    async with aiohttp.ClientSession(connector=aiohttp.TCPConnector(ssl=ssl_context)) as session:
        tasks = []
        for job in data:
            job_id = job.get('id', 'N/A')
            
            # ê° ê³µê³ ì— ëŒ€í•´ 3ê°œì˜ í•„ë“œë¥¼ ë™ì‹œì— LLMì— ìš”ì²­í•˜ëŠ” ì‘ì—…ì„ ìƒì„±
            llm_tasks = [
                tokenize_with_llm(session, semaphore, job.get('main_tasks', ''), "ì£¼ìš” ì—…ë¬´", job_id),
                tokenize_with_llm(session, semaphore, job.get('qualifications', ''), "ìê²© ìš”ê±´", job_id),
                tokenize_with_llm(session, semaphore, job.get('preferences', ''), "ìš°ëŒ€ ì‚¬í•­", job_id)
            ]
            tasks.append(asyncio.gather(*llm_tasks))

        # tqdmìœ¼ë¡œ ì „ì²´ ê³µê³  ì²˜ë¦¬ ì§„í–‰ë¥  í‘œì‹œ
        all_results = await tqdm.gather(*tasks, desc="LLM í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘")

    processed_data = []
    for i, job in enumerate(data):
        main_tasks_skills, required_skills, preferred_skills = all_results[i]
        
        cleaned_job = {'id': job.get('id')}
        
        # í›„ì²˜ë¦¬ ë¡œì§ ì ìš©
        for key, skills_list in zip(['main_tasks_skills', 'required_skills', 'preferred_skills'], [main_tasks_skills, required_skills, preferred_skills]):
            seen_keywords = set()
            unique_cleaned_list = []
            for keyword in skills_list:
                cleaned_keyword = clean_and_standardize_keyword(keyword)
                if cleaned_keyword and cleaned_keyword not in seen_keywords:
                    unique_cleaned_list.append(cleaned_keyword)
                    seen_keywords.add(cleaned_keyword)
            cleaned_job[key] = unique_cleaned_list
            
        processed_data.append(cleaned_job)

    # ìµœì¢… ê²°ê³¼ íŒŒì¼ ì €ì¥
    filename = generate_timestamped_filename("tokenized_jobs")
    file_path = save_json_data(processed_data, filename)
    print(f"\nğŸ‰ ì„±ê³µ! ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ê²°ê³¼ê°€ '{file_path}' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return file_path

# --- Airflow ì‹¤í–‰ í•¨ìˆ˜ ---
def tokenize_and_post_process_jobs(ti):
    """Airflowê°€ í˜¸ì¶œí•˜ëŠ” ë©”ì¸ ë˜í¼ í•¨ìˆ˜"""
    input_path = ti.xcom_pull(task_ids='crawl_content_task', key='return_value')
    if not input_path:
        raise ValueError("XComìœ¼ë¡œë¶€í„° ì›ë³¸ ë°ì´í„° íŒŒì¼ ê²½ë¡œë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    
    return asyncio.run(process_jobs_main(input_path))