import json
import pandas as pd

# Airflow í™˜ê²½ì— ë§ê²Œ utils í•¨ìˆ˜ë¥¼ ì„í¬íŠ¸í•©ë‹ˆë‹¤.
from utils import save_json_data, generate_timestamped_filename, load_json_data

# âœ… DAG íŒŒì¼ì´ ì°¾ëŠ” ì´ë¦„ì¸ 'preprocess_and_save_data'ë¡œ í•¨ìˆ˜ ì´ë¦„ì„ ë³€ê²½í–ˆìŠµë‹ˆë‹¤.
def preprocess_and_save_data(ti):
    """
    Airflow Task: ì´ì „ Taskì˜ ê²°ê³¼(JSON íŒŒì¼)ë¥¼ XComìœ¼ë¡œ ë°›ì•„ ì „ì²˜ë¦¬í•˜ê³ ,
    ê·¸ ê²°ê³¼ë¥¼ DBì— ë§ëŠ” ìµœì¢… JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    """
    # 1. ì´ì „ ìŠ¤í¬ë˜í•‘ Taskê°€ XComìœ¼ë¡œ ë„˜ê²¨ì¤€ íŒŒì¼ ê²½ë¡œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    raw_data_path = ti.xcom_pull(task_ids='scrape_boottent_task', key='return_value')
    if not raw_data_path:
        raise ValueError("XComìœ¼ë¡œë¶€í„° ì›ë³¸ ë°ì´í„° íŒŒì¼ ê²½ë¡œë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    print(f"âœ… ì›ë³¸ ë°ì´í„° íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤: {raw_data_path}")
    raw_data = load_json_data(raw_data_path)
    if not raw_data:
        print("ì „ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    # 2. ë¡œë“œí•œ ë°ì´í„°ë¥¼ pandas DataFrameìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    df = pd.json_normalize(raw_data)
    print(f"ë¡œë“œ ì™„ë£Œ: ì´ {len(df)}ê°œì˜ ì›ë³¸ ë°ì´í„°")

    # 3. ìµœì¢… í…Œì´ë¸” ì»¬ëŸ¼ì— ë§ê²Œ ë°ì´í„°ë¥¼ ì„ íƒí•˜ê³  ì´ë¦„ì„ ë³€ê²½í•©ë‹ˆë‹¤.
    column_mapping = {
        'course_name': 'name',
        'tech_stack': 'skill_description',
        'company': 'company',
        'program_course': 'program_course',
        'start_date': 'start_date',
        'end_date': 'end_date',
        'deadline': 'deadline',
        'location': 'location',
        'onoff': 'onoff',
        'participation_time': 'participation_time',
        'status': 'status'
    }
    
    source_columns = [col for col in column_mapping.keys() if col in df.columns]
    df_processed = df[source_columns].copy()
    df_processed.rename(columns=column_mapping, inplace=True)

    # 4. ë°ì´í„°ë¥¼ ë³€í™˜í•˜ê³  ìƒˆë¡œìš´ ì»¬ëŸ¼ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
    df_processed.insert(0, 'id', range(1, len(df_processed) + 1))
    df_processed['type'] = 'ë¶€íŠ¸ìº í”„'
    df_processed['skill_description'] = df_processed['skill_description'].apply(
        lambda x: json.dumps(x, ensure_ascii=False) if isinstance(x, list) and x else '[]'
    )

    # 5. ë°ì´í„°ë¥¼ ì •ì œí•©ë‹ˆë‹¤.
    required_columns = ['name', 'start_date']
    original_count = len(df_processed)
    df_processed.dropna(subset=required_columns, inplace=True)
    
    if original_count > len(df_processed):
        print(f"ğŸ§¹ í•„ìˆ˜ ê°’ì´ ì—†ì–´ {original_count - len(df_processed)}ê°œì˜ ë°ì´í„°ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")

    # 6. ìµœì¢… ì»¬ëŸ¼ ìˆœì„œë¥¼ ì •ì˜í•˜ê³  ì ìš©í•©ë‹ˆë‹¤.
    final_columns_order = [
        'id', 'name', 'type', 'skill_description', 'company', 'program_course',
        'start_date', 'end_date', 'deadline', 'location', 'onoff',
        'participation_time', 'status'
    ]
    final_columns_in_df = [col for col in final_columns_order if col in df_processed.columns]
    df_final = df_processed[final_columns_in_df]

    # 7. ì „ì²˜ë¦¬ëœ ìµœì¢… ë°ì´í„°ë¥¼ ìƒˆë¡œìš´ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    if not df_final.empty:
        preprocessed_data = df_final.to_dict(orient='records')
        filename = generate_timestamped_filename("preprocessed_bootcamps_final")
        file_path = save_json_data(preprocessed_data, filename)
        
        print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ! ì´ {len(preprocessed_data)}ê°œì˜ ë°ì´í„°ë¥¼ ë‹¤ìŒ ê²½ë¡œì— ì €ì¥í–ˆìŠµë‹ˆë‹¤: {file_path}")
        
        return file_path
    else:
        print("ì „ì²˜ë¦¬ í›„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None