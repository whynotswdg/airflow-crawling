import json
import pandas as pd
from airflow.providers.postgres.hooks.postgres import PostgresHook
from utils import load_json_data, save_json_data, generate_timestamped_filename

def process_and_send_to_postgres(ti):
    """
    Airflow Task: ì—¬ëŸ¬ ì´ì „ Taskë“¤ì˜ ê²°ê³¼(JSON íŒŒì¼)ë¥¼ XComìœ¼ë¡œ ë°›ì•„
    ìµœì¢… ë°ì´í„°ë¥¼ ê°€ê³µí•œ í›„, JSON íŒŒì¼ë¡œ ì €ì¥í•˜ê³  PostgreSQL DBì—ë„ ì „ì†¡í•©ë‹ˆë‹¤.
    """
    # --- ë°ì´í„° ë¡œë“œ (ì´ì „ê³¼ ë™ì¼) ---
    clustered_path = ti.xcom_pull(task_ids='clustering_jobs_task', key='return_value')
    keyword_path = ti.xcom_pull(task_ids='post_process_tokens_task', key='return_value')

    if not clustered_path or not keyword_path:
        raise ValueError("XComìœ¼ë¡œë¶€í„° í´ëŸ¬ìŠ¤í„°ë§ ë˜ëŠ” í† í°í™” ë°ì´í„° íŒŒì¼ ê²½ë¡œë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    print(f"í´ëŸ¬ìŠ¤í„°ë§ ë°ì´í„° ë¡œë“œ: {clustered_path}")
    print(f"í‚¤ì›Œë“œ ë°ì´í„° ë¡œë“œ: {keyword_path}")

    clustered_data = pd.DataFrame(load_json_data(clustered_path))
    keyword_data = pd.DataFrame(load_json_data(keyword_path))

    hook = PostgresHook(postgres_conn_id='postgres_jobs_db')

    try:
        sql = "SELECT id, job_name FROM job_roles"
        job_required_skills = hook.get_pandas_df(sql=sql)
        print(f"âœ… DBì—ì„œ {len(job_required_skills)}ê°œì˜ ì§ë¬´ ì¹´í…Œê³ ë¦¬ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ğŸš¨ DBì—ì„œ 'job_required_skills' í…Œì´ë¸”ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise

    # --- ë°ì´í„° ì „ì²˜ë¦¬ ---
    print("ë°ì´í„° ë³‘í•© ë° ì „ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    merged_data = clustered_data.merge(keyword_data, on='id', how='left')
    
    merged_data.drop_duplicates(subset=['id'], keep='first', inplace=True)

    # [ìˆ˜ì •] ì›ë³¸ ì½”ë“œì˜ 'job_category' ëŒ€ì‹  í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ì¸ 'representative_category'ë¥¼ ì‚¬ìš©
    job_required_skills.rename(columns={"id": "job_required_skill_id", 'job_name': 'representative_category'}, inplace=True)
    join_data = merged_data.merge(job_required_skills[["representative_category", "job_required_skill_id"]], on='representative_category', how='left')
    join_data['job_required_skill_id'] = join_data['job_required_skill_id'].astype('Int64').where(pd.notnull(join_data['job_required_skill_id']), None)

    # ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°
    join_data.drop(columns=["representative_category", "job_category", "cluster"], inplace=True, errors="ignore")
    
    # [ìˆ˜ì •] 'full_embedding'ì„ í¬í•¨í•œ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ ì»¬ëŸ¼ë“¤ì„ JSON ë¬¸ìì—´ë¡œ ë³€í™˜
    for col in ["required_skills", "preferred_skills", "main_tasks_skills", "full_embedding"]:
        if col in join_data.columns:
            join_data[col] = join_data[col].apply(lambda x: json.dumps(x if isinstance(x, list) else []))

    join_data["address"] = join_data["address"].fillna("")
    
    # [ìˆ˜ì •] ìµœì¢… ì»¬ëŸ¼ ëª©ë¡ì— 'full_embedding' ì¶”ê°€
    target_columns = [
        'id', 'title', 'company_name', 'size', 'address', 'job_required_skill_id',
        'employment_type', 'applicant_type', 'posting_date', 'deadline',
        'main_tasks', 'qualifications', 'preferences', 'tech_stack',
        'required_skills', 'preferred_skills', 'main_tasks_skills', 'full_embedding'
    ]
    final_data = join_data[[col for col in target_columns if col in join_data.columns]]

    # --- (ë””ë²„ê¹…ìš©) ë¡œì»¬ íŒŒì¼ ì €ì¥ ---
    final_data_list = final_data.to_dict(orient='records')
    debug_filename = generate_timestamped_filename("final_postgres_payload")
    save_json_data(final_data_list, debug_filename)
    print(f"ğŸ” ë””ë²„ê¹…ìš© ìµœì¢… ë°ì´í„° ì €ì¥ ì™„ë£Œ: /opt/airflow/data/{debug_filename}")

    # --- DB ì €ì¥ ë¡œì§ (ì´ì „ê³¼ ë™ì¼) ---
    try:
        existing_ids_df = hook.get_pandas_df(sql="SELECT id FROM job_posts")
        if not existing_ids_df.empty:
            existing_ids_set = set(existing_ids_df['id'])
            mask = ~final_data['id'].isin(existing_ids_set)
            new_data_to_insert = final_data.loc[mask].copy()
        else:
            new_data_to_insert = final_data.copy()
            
        print(f"ê¸°ì¡´ ë°ì´í„°ì™€ ë¹„êµ í›„, {len(new_data_to_insert)}ê°œì˜ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.")
    except Exception as e:
        print(f"ğŸš¨ DBì—ì„œ ê¸°ì¡´ IDë¥¼ í™•ì¸í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ëª¨ë“  ë°ì´í„°ë¥¼ ì €ì¥ ì‹œë„í•©ë‹ˆë‹¤.")
        new_data_to_insert = final_data

    if new_data_to_insert.empty:
        print("ìƒˆë¡­ê²Œ ì¶”ê°€í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # new_data_to_insert.replace({pd.NA: None}, inplace=True)
    if 'id' in new_data_to_insert.columns:
        new_data_to_insert['id'] = new_data_to_insert['id'].astype(object)
    if 'job_required_skill_id' in new_data_to_insert.columns:
        new_data_to_insert['job_required_skill_id'] = new_data_to_insert['job_required_skill_id'].astype(object)

    try:
        rows_to_insert = list(new_data_to_insert.itertuples(index=False, name=None))
        target_fields = list(new_data_to_insert.columns)
        
        hook.insert_rows(
            table="job_posts",
            rows=rows_to_insert,
            target_fields=target_fields,
            commit_every=1000
        )
        print(f"ğŸ‰ ì„±ê³µ! {len(new_data_to_insert)}ê°œì˜ ë°ì´í„°ê°€ 'job_posts' í…Œì´ë¸”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ğŸš¨ ìµœì¢… ë°ì´í„°ë¥¼ DBì— ì €ì¥í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise