import json
import pandas as pd
from sqlalchemy import text
from airflow.providers.postgres.hooks.postgres import PostgresHook
from utils import load_json_data

def process_and_send_to_postgres(ti):
    """
    Airflow Task: ì—¬ëŸ¬ ì´ì „ Taskë“¤ì˜ ê²°ê³¼(JSON íŒŒì¼)ë¥¼ XComìœ¼ë¡œ ë°›ì•„
    ìµœì¢… ë°ì´í„°ë¥¼ ê°€ê³µí•œ í›„ PostgreSQL DBì— ì €ì¥í•©ë‹ˆë‹¤.
    """
    # 1. ì´ì „ Taskë“¤ë¡œë¶€í„° íŒŒì¼ ê²½ë¡œë¥¼ XComìœ¼ë¡œ ë°›ì•„ì˜µë‹ˆë‹¤.
    clustered_path = ti.xcom_pull(task_ids='clustering_jobs_task', key='return_value')
    keyword_path = ti.xcom_pull(task_ids='tokenize_jobs_task', key='return_value')

    if not clustered_path or not keyword_path:
        raise ValueError("XComìœ¼ë¡œë¶€í„° í´ëŸ¬ìŠ¤í„°ë§ ë˜ëŠ” í† í°í™” ë°ì´í„° íŒŒì¼ ê²½ë¡œë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    print(f"í´ëŸ¬ìŠ¤í„°ë§ ë°ì´í„° ë¡œë“œ: {clustered_path}")
    print(f"í‚¤ì›Œë“œ ë°ì´í„° ë¡œë“œ: {keyword_path}")

    clustered_data = pd.DataFrame(load_json_data(clustered_path))
    keyword_data = pd.DataFrame(load_json_data(keyword_path))

    # 2. Airflow Connectionì„ ì‚¬ìš©í•˜ì—¬ DB ì—”ì§„ ìƒì„±
    hook = PostgresHook(postgres_conn_id='postgres_jobs_db')
    engine = hook.get_sqlalchemy_engine()

    # 3. DBì—ì„œ job_required_skills í…Œì´ë¸” ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    try:
        # [ìˆ˜ì •] Connection ëŒ€ì‹  engine ê°ì²´ë¥¼ ì§ì ‘ ì „ë‹¬í•©ë‹ˆë‹¤.
        job_required_skills = pd.read_sql("SELECT id, job_name FROM job_required_skills", engine)
        print(f"âœ… DBì—ì„œ {len(job_required_skills)}ê°œì˜ ì§ë¬´ ì¹´í…Œê³ ë¦¬ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ğŸš¨ DBì—ì„œ 'job_required_skills' í…Œì´ë¸”ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise
    
    # --- ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘ ---
    print("ë°ì´í„° ë³‘í•© ë° ì „ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    # 4. clustered_dataì™€ keyword_data ë³‘í•©
    merged_data = clustered_data.merge(keyword_data, on='id', how='left')

    # 5. ì§ë¬´ ì¹´í…Œê³ ë¦¬ ID ì¡°ì¸
    job_required_skills.rename(columns={"id": "job_required_skill_id", 'job_name': 'job_category'}, inplace=True)
    join_data = merged_data.merge(job_required_skills[["job_category", "job_required_skill_id"]], on='job_category', how='left')

    # 6. ìµœì¢… ë°ì´í„°í”„ë ˆì„ ìƒì„± ë° í›„ì²˜ë¦¬
    join_data.drop(columns=["job_category", "cluster"], inplace=True, errors="ignore")
    
    for col in ["required_skills", "preferred_skills", "main_tasks_skills"]:
        # NaN ê°’ì„ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬ í›„ JSON ë¬¸ìì—´ë¡œ ë³€í™˜
        join_data[col] = join_data[col].apply(lambda x: json.dumps(x if isinstance(x, list) else []))

    join_data["address"] = join_data["address"].fillna("")
    
    # DB í…Œì´ë¸” ì»¬ëŸ¼ ìˆœì„œì— ë§ê²Œ ì¬ì •ë ¬ (embedding ì»¬ëŸ¼ì€ ì œì™¸í•˜ê³  ë¡œë“œ)
    target_columns = [
        'id', 'title', 'company_name', 'size', 'address', 'job_required_skill_id',
        'employment_type', 'applicant_type', 'posting_date', 'deadline',
        'main_tasks', 'qualifications', 'preferences', 'tech_stack',
        'required_skills', 'preferred_skills', 'main_tasks_skills'
    ]
    # 'created_at'ì€ DBì—ì„œ ìë™ìœ¼ë¡œ ìƒì„±ë˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì œì™¸í•©ë‹ˆë‹¤.
    final_data = join_data[[col for col in target_columns if col in join_data.columns]]

    # 7. DBì— ì´ë¯¸ ìˆëŠ” IDëŠ” ì œì™¸í•˜ì—¬ ì¤‘ë³µ ë°©ì§€
    try:
        with engine.connect() as conn:
            existing_ids = pd.read_sql(text("SELECT id FROM job_posts"), conn)
            existing_ids_set = set(existing_ids['id'])
        
        mask = ~final_data['id'].isin(existing_ids_set)
        new_data_to_insert = final_data[mask]
        print(f"ê¸°ì¡´ ë°ì´í„°ì™€ ë¹„êµ í›„, {len(new_data_to_insert)}ê°œì˜ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.")
    except Exception as e:
        print(f"ğŸš¨ DBì—ì„œ ê¸°ì¡´ IDë¥¼ í™•ì¸í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        new_data_to_insert = final_data # ì˜¤ë¥˜ ì‹œ ì „ì²´ ë°ì´í„°ë¥¼ ì €ì¥ ì‹œë„

    if new_data_to_insert.empty:
        print("ìƒˆë¡­ê²Œ ì¶”ê°€í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 8. ìµœì¢… ë°ì´í„°ë¥¼ PostgreSQL 'job_posts' í…Œì´ë¸”ì— ì €ì¥
    try:
        new_data_to_insert.to_sql('job_posts', engine, if_exists='append', index=False)
        print(f"ğŸ‰ ì„±ê³µ! {len(new_data_to_insert)}ê°œì˜ ë°ì´í„°ê°€ 'job_posts' í…Œì´ë¸”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ğŸš¨ ìµœì¢… ë°ì´í„°ë¥¼ DBì— ì €ì¥í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise